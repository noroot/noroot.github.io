<!doctype html>
<html lang="en">
    <head>
	<!-- Required meta tags -->
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
	<link rel="shortcut icon" href="/images/favicon.png" type="image/x-icon">
	<link rel="stylesheet" href="/assets/main.css">
	<link rel="stylesheet" href="/assets/highlight.css">
	<title>Архитектура moskva.com | False!True</title>
        
        
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=UA-28956735-2"></script>
	<script>
	 window.dataLayer = window.dataLayer || [];
	 function gtag(){dataLayer.push(arguments);}
	 gtag('js', new Date());

	 gtag('config', 'UA-28956735-2');
	</script>
    </head>
    <body>
	<nav class="navbar navbar-expand-md navbar-dark bg-dark fixed-top mb-5">
	    <div class="container">
		<a class="navbar-brand" href="/"><span style="color:lightgreen"> False!</span><span style="color:cyan">True</span></a>
		<button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarsEx" aria-controls="navbarsEx" aria-expanded="false" aria-label="Toggle navigation">
		    <span class="navbar-toggler-icon"></span>
		</button>

		<div class="collapse navbar-collapse" id="navbarsEx">
		    <ul class="navbar-nav mr-auto">
			<li class="nav-item active">
			    <a class="nav-link" href="#">Home</a>
			</li>
			<li class="nav-item">
			    <a class="nav-link" href="/archive/">Archive</a>
			</li>
			<li class="nav-item">
			    <a class="nav-link" href="/categories/books/">Bookshelf</a>
			</li>
			<li class="nav-item">
			    <a class="nav-link" href="https://slides.falsetrue.io">Slides</a>
			</li>
			<li class="nav-item">
			    <a class="nav-link" href="/about/">About</a>
			</li>
		    </ul>
		    <ul class="nav navbar-nav navbar-right">
			<li>
		            <a href="http://feeds.feedburner.com/OvermindsHive" class="om-rss-link">Subscirbe <img src="/images/rss.png" /> RSS</a>
			</li>
                    </ul>
		</div>
	    </div>
	</nav>
	
	<div class="container mt-3 pt-5">
            <div class="row mb-5" style="border-left: 4px solid lightgray">
    <div class="col">
	<time datetime="2008-01-01 18:00:00 +0300">January 1, 2008</time>    
	<h1>Архитектура moskva.com</h1>
	<p class="meta">
	    
	</p>
	<div class="post-categories pb-3">
	    Categories:
	    
	    <a href="/categories/admin/" class="badge badge-success">admin</a>
	    
	</div>
	<div class="content">
	    <p>Проект москва.ком представял из себя главный городской портал Москвы. С посещамостью 100к-300к хостов в день.</p>

<p>Используемые технологии 
Debian Linux (etch), Nginx 0.4-0.6, Apache 2.X, PHP 5, MySQL 5.0, Memcache, Xdebug, DRBD.</p>

<h2 id="теперь-по-порядку">Теперь по порядку</h2>

<p>Изначально сайт работал на 3-х серверах, каждый из которых выполнял свою роль, frontend, backend, database + backups, для организации fail-over решения, этого было недостаточно, точнее fail-over решение вообше отсутствовало в каком либо виде. 
По скольку планировалось обслуживать большой трафик посетителей, было решено пересмотреть мощности и реализовать highload и failover систему.</p>

<h2 id="hardware">Hardware</h2>
<p>Было приобретено:</p>

<ul>
  <li>2 x <strong>Dell SC1435</strong> (2 x Duo Core AMD 2200Mhz / 2GB RAM / 70GB SATA) <a href="http://www.dell.com/downloads/emea/products/pedge/en/SC1435_Spec_Sheet%20.pdf">Спецификация</a></li>
  <li>4 x <strong>Dell PowerEdgeTM 2950</strong> (4 x Core Duo Intel(R) Xeon(TM) MP CPU 3.16GHz / 16GB RAM / 300GB SCSI) <a href="http://www.dell.com/downloads/global/products/pedge/en/PE2950_SS_072007.pdf">Спецификация</a></li>
  <li>
    <p>2 x <strong>SuperMicro</strong> (4 x Intel(R) Xeon(R) CPU 5130  @ 2.00GHz / 3GB RAM / 16 x 750GB SCSI Дисков + RAID 50 / 60 )</p>
  </li>
  <li>2 x KVM ALTUSEN 16 ports</li>
  <li>2 x POWER CONTROL ALTUSEN 16 ports</li>
</ul>

<p>Всего получилось 10 серверов и еще один маршрутизатор cisco express 500, и все это железо предстояло подключить и настроить. подробнее об этом дальше.</p>

<h2 id="highload-n-failover">Highload n Failover</h2>
<p>10 делились на 3 логические группы (уровни):</p>

<p>На каждом уровне было как минимум 2 сервере, для обеспечения отказоустойчивости 99,9% (3 уровень отказоустойчивости)</p>

<h3 id="1-уровень-loadbalance-и-статический-контент">1. Уровень Loadbalance и статический контент</h3>

<p>Это балансировщик нагрузки, в качестве которого выступал nginx(тогда еще версии 0.3) и failover демон heartbeat который следил за серверами и в случае отказа, переключал нагрузку на доступный сервер.</p>

<h3 id="2-уровень-backend-workers-and-database">2. Уровень Backend workers and database</h3>

<p>На этом уровене было 4 сервера. 2 из которых были чистыми application серверами на apache для php, и принимали запросы только с первого уровня.
2 другие сервера это Mysql база данных с Master-Slave репликацией и резервным копированием со Slave</p>

<h3 id="3-уровень-storage">3. Уровень Storage</h3>

<p>Два огромных сервер по 16 x 750GB SCSI дисков на каждом и RAID 50 итого на каждом получалось по 2ТБ хранилища.
Но не все так просто, была задача обеспечить отказоустойчивость, то есть если один storage полетит надо было переключать на второй и встал вопрос, как при этом синхронизировать данные при больших обьемах, а это были пользовательские файлы, в основном видео и фотографии от 1MB до 1000MB. После всяких тестов, было принято решение использовать <a href="http://drbd.org">DRBD</a></p>

<p>Суть <strong>DRBD</strong> показан на этой картинке, вкратце это демон который в реалтайме синхронизирует диски, работет он на уровне драйвера</p>

<p><img src="http://www.drbd.org/uploads/pics/overview_02.gif" /></p>

<p>Вещь эта очень сложная, и на тот момент в первых версиях, нельзя было держать оба раздела смонтированным, смонтированным должен был быть только один раздел, поэтому на всякий случай имелся bash скрипт, который умел менять местами эти разделы.</p>

<p>Вот так выглядел эта отказоустойчивая система в процессе разработки. (На фото нет двух больших 4U Supermicro серверов)</p>

<p><img src="/images/msk-cluster.jpg" /></p>

<h4 id="теперь-собственно-сама-архитектура">Теперь собственно сама архитектура</h4>

<h6 id="linux">Linux</h6>
<ul>
  <li>Был выбран стабильный дистрибутив Debian(Etch) Linux</li>
  <li>Проблемы возникли с несертифицированных железом, но проблемы исправимые, пришлось повозится с драйверами для SCSI контроллера обоих storage серверов</li>
</ul>

<h5 id="php">PHP</h5>
<ul>
  <li>Использовался потомучто “так исторически сложилось””</li>
  <li>Поначалу все работало быстро, при большой нагрузке понадобились оптимизаторы кода <strong>APC</strong> на тот момент был самым быстрым</li>
  <li>Отладка с помощью Xdebug</li>
</ul>

<h5 id="mysql">MySQL</h5>
<ul>
  <li>Очень плотное использование MySQL хранимые процедуры и триггеры</li>
  <li>Репликация помогла избежать dead locks, и смягчала нагрузку на сервисы которые работали с данными readonly</li>
  <li>Репликация не раз спасала при восстановлении данных</li>
  <li>Настройка и управление репликацие требует полного понимания просиходящего</li>
  <li>Постоянный мониторинг с помощью MyTop</li>
</ul>

<h5 id="apache">Apache</h5>
<ul>
  <li>Использовался mod_rewrite</li>
  <li>Постоянный мониторинг server-status для выяснения проблем</li>
</ul>

<h5 id="memcache">Memcache</h5>
<ul>
  <li>Все что мало изменялось хранилось и часто использовалось в memcache</li>
</ul>

<h5 id="nfs">NFS</h5>
<ul>
  <li>Код хранился на одном сервер и распределялся на другие через NFS</li>
  <li>Также по NFS ходили большие пользовтельские данные video и фотографии, которые отдавались с первого уровня балансерами</li>
  <li>Правильно настроенный NFS отлично справлялся со своей задачей очень важно тонко настраивать такие вещи как NFS</li>
</ul>

<h5 id="сеть">Сеть</h5>
<ul>
  <li>Каждый сервер имел 2 x 1Gb карты чтобы обеспечивало безполезенную передачу данных с минимальной задержкой по локальной сети</li>
</ul>

<h5 id="nginx">Nginx</h5>

<ul>
  <li>Так обеспечивалась отдача статического контента, в частности изображений</li>
</ul>

<figure class="highlight"><pre><code class="language-nginx" data-lang="nginx">	<span class="k">location</span> <span class="n">/p/</span> <span class="p">{</span>
        <span class="kn">expires</span> <span class="s">max</span><span class="p">;</span>
        <span class="kn">add_header</span> <span class="s">Cache-Control</span> <span class="s">public</span><span class="p">;</span>
                
        <span class="kn">if</span> <span class="s">(</span><span class="nv">$request_filename</span> <span class="p">~</span><span class="sr">*</span> <span class="s">".*</span><span class="err">\</span><span class="s">.php")</span> <span class="p">{</span>
                <span class="kn">return</span> <span class="mi">403</span><span class="p">;</span>
        <span class="p">}</span>
    
        <span class="kn">if</span> <span class="s">(!-f</span> <span class="nv">$request_filename</span><span class="s">)</span>
        <span class="p">{</span>
                <span class="kn">break</span><span class="p">;</span>
                <span class="kn">proxy_pass</span> <span class="s">http://backend-grp-1</span><span class="p">;</span>
        <span class="p">}</span>
	<span class="p">}</span></code></pre></figure>

<h5 id="kvm-и-powerswitch">KVM и Powerswitch</h5>

<ul>
  <li>Огромную роль сыграло дополнительно железо в частности KVM и PowerSwitch, с помошью KVM нераз удавалось удаленно установить проблемы в датацентре по вине провайдера, в частности проблемы маршрутизации из-за которых нероботал сайт.</li>
</ul>

<h4 id="итого">Итого</h4>

<p>Все работало, стабильно,всего пришлось 2 раза восстанавливать базу данных, один раз по случайсти один из разработчиков удалил таблица, второй раз из-за MyISAM corruption. DRBD переключение зеркала, также пришлось использовать один раз, но только для проверки накопившехся данных и для того чтобы сделать с него бекап на отдельный диск. Сильно помогла поддержка KVM и Powercontrol, позволял понять что неисправно и заходить в bios RAID контроллеров.</p>


	</div>
	<section>
	    <h2>Comments</h2>
	    <div id="disqus_thread"></div>
	</section>
	<script type="text/javascript">
         /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
         var disqus_shortname = 'om6'; // required: replace example with your forum shortname

         /* * * DON'T EDIT BELOW THIS LINE * * */
         (function() {
             var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
             dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
             (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
         })();
	</script>
	<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
	<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>    	
    </div>
</div>

	</div>
	<footer class="footer">
	    Dmitry Rodichev &copy; 2003 - 2019 <br>
	    Копирование и распрастрнение страницы разрешено только в оригинальном не изменном виде, включая это сообщение..<br>
	    Verbatim copying and redistribution of this entire page are permitted provided this notice is preserved. <br>
	    Verbatim copying and redistribution of any of the photos in the photos subdirectory is permitted under the <a href="http://creativecommons.org/licenses/by-nd/3.0/" rel="nofollow"> Creative Commons Noderivs license version 3.0</a> or later.
	</footer>
	
	<script src="/assets/vendor/jquery.min.js"></script>
	<script src="/assets/vendor/popper.min.js"></script>
	<script src="/assets/vendor/bootstrap.min.js"></script>
    </body>
</html>
